# Tema B: OtimizaÃ§Ã£o de Armazenamento e Consulta com PySpark

**MBA em Engenharia de Dados - Data Collection & Storage**

Este projeto demonstra a importÃ¢ncia da otimizaÃ§Ã£o de armazenamento e consulta em um ambiente de Big Data, comparando diferentes formatos de arquivo e propondo uma estratÃ©gia de ciclo de vida de dados. A soluÃ§Ã£o foi totalmente containerizada com Docker para garantir reprodutibilidade e facilidade de uso no GitHub Codespaces.

## ğŸš€ Objetivos

- **Comparar Formatos:** Analisar o impacto de formatos de arquivo (CSV, JSON, Parquet, ORC) no tamanho de armazenamento e na performance de queries.
- **Analisar Performance:** Medir o tempo de leitura, filtro e agregaÃ§Ã£o para cada formato.
- **Propor Ciclo de Vida:** Desenvolver uma estratÃ©gia de ciclo de vida (Hot, Warm, Cold) para otimizar custos.
- **Containerizar SoluÃ§Ã£o:** Empacotar a aplicaÃ§Ã£o com Docker para execuÃ§Ã£o em qualquer ambiente, incluindo GitHub Codespaces.

## ğŸ“Š Arquitetura da SoluÃ§Ã£o

```mermaid
graph TD
    subgraph "Ambiente Docker"
        A[Dockerfile] --> B(Imagem Docker com Spark 3.5.0)
        C[docker-compose.yml] --> D{Container Spark}
    end

    subgraph "Pipeline de AnÃ¡lise"
        E[GeraÃ§Ã£o de Dataset] --> F{1M Registros IoT}
        F --> G[Salvar em CSV]
        F --> H[Salvar em JSON]
        F --> I[Salvar em Parquet]
        F --> J[Salvar em ORC]
        
        subgraph "AnÃ¡lise de Performance"
            G --> K[Leitura e Queries]
            H --> K
            I --> K
            J --> K
        end
        
        K --> L[RelatÃ³rio Comparativo]
    end

    D -- Monta volumes --> E
    L -- Salva em --> M[output/relatorio.json]
```

## ğŸ“Š Dataset IncluÃ­do

Este projeto inclui um **dataset prÃ©-gerado** de 1 milhÃ£o de registros (~87 MB) para acelerar a execuÃ§Ã£o:

- **Arquivo:** `data/tema_b_sensores_iot.csv`
- **Registros:** 1.000.000 leituras de sensores IoT
- **Tamanho:** ~87 MB
- **PerÃ­odo:** Ano de 2024
- **DocumentaÃ§Ã£o:** Veja `data/DATASET_INFO.md` para detalhes completos

**Vantagem:** Ao usar o dataset prÃ©-gerado, a execuÃ§Ã£o leva apenas **~3-5 minutos** ao invÃ©s de ~10 minutos (geraÃ§Ã£o + anÃ¡lise).

## ğŸ› ï¸ Tecnologias Utilizadas

- **Linguagem:** Python 3.11
- **Processamento:** Apache Spark 3.5.0
- **ContainerizaÃ§Ã£o:** Docker, Docker Compose
- **Formatos Analisados:** CSV, JSON, Parquet (Snappy), ORC (Snappy)
- **Bibliotecas Python:** PySpark, Pandas, Matplotlib, Seaborn

## ğŸ“ Estrutura de DiretÃ³rios

```
/tema_b_github
â”œâ”€â”€ Dockerfile             # Define a imagem Docker com Spark e dependÃªncias
â”œâ”€â”€ docker-compose.yml     # Orquestra o container de serviÃ§o
â”œâ”€â”€ README.md              # Este arquivo
â”œâ”€â”€ requirements.txt       # DependÃªncias Python
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ tema_b_otimizacao_docker.py  # Script principal da anÃ¡lise
â”œâ”€â”€ data/                  # DiretÃ³rio para armazenar os datasets gerados
â”œâ”€â”€ output/                # DiretÃ³rio para salvar relatÃ³rios e grÃ¡ficos
â”œâ”€â”€ notebooks/             # (Opcional) Para anÃ¡lises interativas em Jupyter
â””â”€â”€ .devcontainer/         # ConfiguraÃ§Ã£o para GitHub Codespaces
    â””â”€â”€ devcontainer.json
```

## ğŸš€ Como Executar no GitHub Codespaces

1. **Abrir no Codespaces:**
   - Clique no botÃ£o **"Code"** neste repositÃ³rio.
   - Selecione a aba **"Codespaces"**.
   - Clique em **"Create codespace on main"**.

2. **Iniciar o Ambiente:**
   - O GitHub Codespaces irÃ¡ automaticamente construir e iniciar o container Docker usando os arquivos `Dockerfile` e `docker-compose.yml`.
   - Aguarde a conclusÃ£o do processo. VocÃª terÃ¡ um terminal pronto para uso.

3. **Executar a AnÃ¡lise:**
   - No terminal do Codespaces, execute o script principal:
     ```bash
     python3 /app/scripts/tema_b_otimizacao_docker.py
     ```
   - **Nota:** O script detecta automaticamente o dataset prÃ©-gerado em `data/` e o utiliza, economizando tempo de geraÃ§Ã£o.

4. **Verificar os Resultados:**
   - O script irÃ¡ gerar os datasets no diretÃ³rio `/app/data` e o relatÃ³rio final em `/app/output`.
   - VocÃª pode explorar os arquivos diretamente na interface do VS Code.
   - Para visualizar o relatÃ³rio:
     ```bash
     cat /app/output/relatorio_comparativo.json
     ```

## âš™ï¸ Como Executar Localmente (com Docker)

1. **PrÃ©-requisitos:**
   - Docker e Docker Compose instalados.

2. **Construir e Iniciar o Container:**
   ```bash
   docker-compose up -d --build
   ```

3. **Acessar o Container:**
   ```bash
   docker-compose exec spark-tema-b bash
   ```

4. **Executar a AnÃ¡lise:**
   - Dentro do container, execute o script:
     ```bash
     python3 /app/scripts/tema_b_otimizacao_docker.py
     ```

5. **Verificar os Resultados:**
   - Os resultados estarÃ£o nos diretÃ³rios `data/` e `output/` no seu host local, pois foram montados como volumes.

## ğŸ“ˆ Resultados Esperados

O script irÃ¡ gerar um relatÃ³rio detalhado no console e um arquivo JSON com os seguintes resultados:

- **Comparativo de Tamanho:**
  - **CSV:** ~87 MB
  - **JSON:** ~120 MB
  - **Parquet:** ~25 MB (reduÃ§Ã£o de ~71%)
  - **ORC:** ~22 MB (reduÃ§Ã£o de ~75%)

- **Comparativo de Performance:**
  - **Leitura Completa:** Formatos colunares (Parquet/ORC) sÃ£o ~3-4x mais rÃ¡pidos.
  - **Queries com Filtro:** Formatos colunares sÃ£o ~5-10x mais rÃ¡pidos devido ao Predicate Pushdown.
  - **Queries com AgregaÃ§Ã£o:** Formatos colunares sÃ£o ~3-5x mais rÃ¡pidos.

## ğŸ”„ EstratÃ©gia de Ciclo de Vida

O projeto tambÃ©m propÃµe uma estratÃ©gia de ciclo de vida para otimizaÃ§Ã£o de custos:

- **Hot Storage (0-30 dias):**
  - **MÃ­dia:** SSD/NVMe
  - **Formato:** Parquet (Snappy)
  - **Custo:** Alto
  - **Uso:** Dashboards e anÃ¡lises em tempo real.

- **Warm Storage (31-180 dias):**
  - **MÃ­dia:** HDD
  - **Formato:** Parquet (GZIP - maior compressÃ£o)
  - **Custo:** MÃ©dio
  - **Uso:** RelatÃ³rios mensais e anÃ¡lises de tendÃªncia.

- **Cold Storage (181+ dias):**
  - **MÃ­dia:** Object Storage (AWS S3 Glacier, Azure Archive)
  - **Formato:** Parquet (GZIP)
  - **Custo:** Baixo
  - **Uso:** Conformidade regulatÃ³ria e auditorias.

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido para fins educacionais como parte do Projeto Final do MBA em Engenharia de Dados.
